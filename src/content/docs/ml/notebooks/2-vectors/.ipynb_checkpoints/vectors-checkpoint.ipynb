{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dff96d2-a160-491f-842c-c07453d11e82",
   "metadata": {},
   "source": [
    "On the last sheet, we looked at a formulation for regression that worked on single points (a 'pointwise' description). In this sheet we will look at how we can describe linear regression instead in terms of vectors. \n",
    "\n",
    "\n",
    "Recall the formula for simple linear regression:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "\n",
    "Its also important to remember the process for matrix multiplication. Let's start with a very simple example\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    a & 0 \\\\ 0 & b\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x\\\\\n",
    "    y\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What does that mean? A simple intuition is that each column in the matrix describes how each row in the vector should be transformed. We can expand out the result as so:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    xa + 0y\\\\\n",
    "    0a + by\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    ax\\\\\n",
    "    by\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, every row in the matrix corresponds to a row in the resulting vector. An interesting result: if we have only a single row in the matrix, then the result is a single number. Let's look at annother example:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    a & b\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x\\\\\n",
    "    y\\\\\n",
    "\\end{bmatrix} = ax + by\n",
    "$$\n",
    "\n",
    "Now, that might look familiar; if we rename the variables in our vectors a little bit here, something interesting happens:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    w_0 & w_1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    x\\\\\n",
    "\\end{bmatrix} = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "This is exactly the same as our definition of simple linear regression! We have represented regression with vectors!\n",
    "\n",
    "We typically give these vectors names; for example:\n",
    "\n",
    "$$\n",
    "\\text{parameters} = w = \\begin{bmatrix}\n",
    "    w_0\\\\\n",
    "    w_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\text{data} = x = \\begin{bmatrix}\n",
    "1\\\\\n",
    "x\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that to get the multiplication to work like we want, we need to use the transpose of the parameters vector; Thus we can re-write our model as:\n",
    "\n",
    "$$\n",
    "y = w^\\intercal x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67dbec",
   "metadata": {},
   "source": [
    "This is quite useful, but can we go further? What if we tried to compute all our predictions at once? First lets think about the format we want our answer to be in:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    y_1\\\\\n",
    "    y_2\\\\\n",
    "    y_3\\\\\n",
    "    \\vdots\\\\\n",
    "    y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can stack all our values one on top of the other. So then how should our input data look?\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    1 & x_1\\\\\n",
    "    1 & x_2\\\\\n",
    "    1 & x_3\\\\\n",
    "    \\vdots & \\vdots\\\\\n",
    "    1 & x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, if we multiply this by $w$ from above:\n",
    "\n",
    "$$\n",
    "Xw = \\begin{bmatrix}\n",
    "    w_0 + w_1x_1\\\\\n",
    "    w_0 + w_1x_2\\\\\n",
    "    w_0 + w_1x_3\\\\\n",
    "    \\vdots\\\\\n",
    "    w_0 + w_1x_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Which is exactly what we wanted. Note the difference with our earlier definition: its $Xw$, not $w^\\intercal x$. If you don't understand why, try writing out some small matrices and doing the calculation by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5023bfb",
   "metadata": {},
   "source": [
    "# Vectorizing loss\n",
    "\n",
    "How about loss?\n",
    "\n",
    "Well, vectorising error is quite easy; remember, error for a single point is given by:\n",
    "\n",
    "$$t - f(x)$$\n",
    "\n",
    "For our vector model, we can use:\n",
    "\n",
    "$$\n",
    "e = t_n - w^\\intercal x_n\n",
    "$$\n",
    "\n",
    "To compute the mean, we can just sum along the vector\n",
    "\n",
    "$$\n",
    "ME = \\frac{1}{|X|} \\sum_{i =0} ^{|X|} t_i - w^\\intercal x_i\n",
    "$$\n",
    "\n",
    "But something very cool happens when we look at mean squared error. First, note the following identity:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{D} a_i^2 = A^\\intercal A\n",
    "$$\n",
    "\n",
    "Where $a_i$ is a row of $A$.\n",
    "\n",
    "We can exploit this identity in our case like so:\n",
    "\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{|X|} \\sum_{i =0} ^{|X|} (t_i - w^\\intercal x_i)^2 = \\frac{1}{|X|} (t - Xw)^\\intercal (t - Xw)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005d5fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a4390",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29dc97d",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
