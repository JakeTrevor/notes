{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dff96d2-a160-491f-842c-c07453d11e82",
   "metadata": {},
   "source": [
    "On the last sheet, we looked at a formulation for regression that worked on single points (a 'pointwise' description). In this sheet we will look at how we can describe linear regression instead in terms of vectors. \n",
    "\n",
    "\n",
    "Recall the formula for simple linear regression:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "\n",
    "Its also important to remember the process for matrix multiplication. Let's start with a very simple example\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    a & 0 \\\\ 0 & b\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x\\\\\n",
    "    y\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What does that mean? A simple intuition is that each column in the matrix describes how each row in the vector should be transformed. We can expand out the result as so:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    xa + 0y\\\\\n",
    "    0a + by\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    ax\\\\\n",
    "    by\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, every row in the matrix corresponds to a row in the resulting vector. An interesting result: if we have only a single row in the matrix, then the result is a single number. Let's look at annother example:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    a & b\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x\\\\\n",
    "    y\\\\\n",
    "\\end{bmatrix} = ax + by\n",
    "$$\n",
    "\n",
    "Now, that might look familiar; if we rename the variables in our vectors a little bit here, something interesting happens:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    w_0 & w_1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    x\\\\\n",
    "\\end{bmatrix} = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "This is exactly the same as our definition of simple linear regression! We have represented regression with vectors!\n",
    "\n",
    "We typically give these vectors names; for example:\n",
    "\n",
    "$$\n",
    "\\text{parameters} = w = \\begin{bmatrix}\n",
    "    w_0\\\\\n",
    "    w_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\text{data} = x = \\begin{bmatrix}\n",
    "1\\\\\n",
    "x\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that to get the multiplication to work like we want, we need to use the transpose of the parameters vector; Thus we can re-write our model as:\n",
    "\n",
    "$$\n",
    "y = w^\\intercal x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67dbec",
   "metadata": {},
   "source": [
    "This is quite useful, but can we go further? What if we tried to compute all our predictions at once? First lets think about the format we want our answer to be in:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    y_1\\\\\n",
    "    y_2\\\\\n",
    "    y_3\\\\\n",
    "    \\vdots\\\\\n",
    "    y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can stack all our values one on top of the other. So then how should our input data look?\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    1 & x_1\\\\\n",
    "    1 & x_2\\\\\n",
    "    1 & x_3\\\\\n",
    "    \\vdots & \\vdots\\\\\n",
    "    1 & x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, if we multiply this by $w$ from above:\n",
    "\n",
    "$$\n",
    "Xw = \\begin{bmatrix}\n",
    "    w_0 + w_1x_1\\\\\n",
    "    w_0 + w_1x_2\\\\\n",
    "    w_0 + w_1x_3\\\\\n",
    "    \\vdots\\\\\n",
    "    w_0 + w_1x_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Which is exactly what we wanted. Note the difference with our earlier definition: its $Xw$, not $w^\\intercal x$. If you don't understand why, try writing out some small matrices and doing the calculation by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5023bfb",
   "metadata": {},
   "source": [
    "# Vectorizing loss\n",
    "\n",
    "How about loss?\n",
    "\n",
    "Well, vectorising error is quite easy; remember, error for a single point is given by:\n",
    "\n",
    "$$t - f(x)$$\n",
    "\n",
    "For our vector model, we can use:\n",
    "\n",
    "$$\n",
    "e = t_n - w^\\intercal x_n\n",
    "$$\n",
    "\n",
    "To compute the mean, we can just sum along the vector\n",
    "\n",
    "$$\n",
    "ME = \\frac{1}{|X|} \\sum_{i =0} ^{|X|} t_i - w^\\intercal x_i\n",
    "$$\n",
    "\n",
    "But something very cool happens when we look at mean squared error. First, note the following identity:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{D} a_i^2 = A^\\intercal A\n",
    "$$\n",
    "\n",
    "Where $a_i$ is a row of $A$.\n",
    "\n",
    "We can exploit this identity in our case like so:\n",
    "\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{|X|} \\sum_{i =0} ^{|X|} (t_i - w^\\intercal x_i)^2 = \\frac{1}{|X|} (t - Xw)^\\intercal (t - Xw)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a140ee",
   "metadata": {},
   "source": [
    "# Vectorising Regression\n",
    "\n",
    "So, hopefully you remember at least some of the vectorisation stuff we just went over; feel free to refer back to it if you need. Lets start nice and easy: given some parameters, put them into the parameter vector $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95d1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744abaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeW(w0:float, w1:float) -> np.array:\n",
    "    return np.array([w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d0154",
   "metadata": {},
   "source": [
    "Hopefully, you didn't have any trouble there. Now lets use that to predict a value for a given data point. \n",
    "\n",
    "Hint: numpy uses `@` for matrix multiplication (and equivalently, for vector dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16df169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = makeW(1, 10)\n",
    "params = makeW(5, 0.5)\n",
    "\n",
    "data.T @ params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ef1c4",
   "metadata": {},
   "source": [
    "So we've handled a very simple case. Let's get a bit more advanced. Here is some more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6875c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5699d5fb",
   "metadata": {},
   "source": [
    "We need to format this data so that our model can work with it. write a function that takes in some 1-dimensional data, and produces the proper vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe55aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 2.],\n",
       "       [1., 3.],\n",
       "       [1., 4.],\n",
       "       [1., 5.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatData(data) -> np.array:\n",
    "    ones = np.ones(len(data))\n",
    "    return np.array([ones, data]).T\n",
    "\n",
    "X = formatData([1, 2, 3, 4, 5])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec73163",
   "metadata": {},
   "source": [
    "Now, predict the values for all of those at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0de2a174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12., 14., 16., 18., 20.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = makeW(10, 2)\n",
    "X @ w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b022f",
   "metadata": {},
   "source": [
    "Nice! Now lets compute the loss for that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5892a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, X, t):\n",
    "    error = (t - X @ w)\n",
    "    return (error @ error) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991a2d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "709bdea0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
